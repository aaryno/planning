name: ðŸ“Š Automated Grading - Python GeoPandas Introduction

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch: # Allow manual trigger

env:
  PYTHON_VERSION: '3.11'
  UV_CACHE_DIR: ~/.cache/uv

jobs:
  grade-assignment:
    name: ðŸŽ¯ Grade GeoPandas Functions
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - name: ðŸ“¥ Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - name: ðŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: âš¡ Install uv (Modern Python Package Manager)
      uses: astral-sh/setup-uv@v2
      with:
        version: "latest"
        enable-cache: true

    - name: ðŸ’¾ Cache uv dependencies
      uses: actions/cache@v3
      with:
        path: ${{ env.UV_CACHE_DIR }}
        key: uv-${{ runner.os }}-${{ hashFiles('pyproject.toml', 'uv.lock') }}
        restore-keys: |
          uv-${{ runner.os }}-

    - name: ðŸ”§ Install System Dependencies
      run: |
        sudo apt-get update -q
        sudo apt-get install -y --no-install-recommends \
          gdal-bin \
          libgdal-dev \
          libgeos-dev \
          libproj-dev \
          libspatialite-dev \
          spatialite-bin
        echo "ðŸ“¦ Spatial libraries installed successfully"

    - name: ðŸ“¦ Install Python Dependencies
      run: |
        echo "ðŸ”„ Installing dependencies with uv..."
        uv sync --group test --group dev
        echo "âœ… Dependencies installed successfully"

        # Verify critical spatial libraries
        echo "ðŸ§ª Verifying spatial libraries installation..."
        uv run python -c "import geopandas; print(f'âœ… GeoPandas {geopandas.__version__}')"
        uv run python -c "import shapely; print(f'âœ… Shapely {shapely.__version__}')"
        uv run python -c "import fiona; print(f'âœ… Fiona {fiona.__version__}')"
        uv run python -c "import pyproj; print(f'âœ… PyProj {pyproj.__version__}')"

    - name: ðŸ—‚ï¸ Verify Assignment Structure
      run: |
        echo "ðŸ“ Checking assignment file structure..."

        # Check required directories
        test -d "src" || { echo "âŒ Missing src/ directory"; exit 1; }
        test -d "tests" || { echo "âŒ Missing tests/ directory"; exit 1; }
        test -d "data" || { echo "âŒ Missing data/ directory"; exit 1; }

        # Check required files
        test -f "src/spatial_basics.py" || { echo "âŒ Missing src/spatial_basics.py"; exit 1; }
        test -f "tests/test_spatial_basics.py" || { echo "âŒ Missing tests/test_spatial_basics.py"; exit 1; }
        test -f "pyproject.toml" || { echo "âŒ Missing pyproject.toml"; exit 1; }

        echo "âœ… Assignment structure verified"

    - name: ðŸ” Validate Python Syntax
      run: |
        echo "ðŸ” Checking Python syntax..."
        uv run python -m py_compile src/spatial_basics.py
        echo "âœ… Python syntax is valid"

    - name: ðŸ“Š Import Test - Check Function Definitions
      run: |
        echo "ðŸ“Š Testing function imports..."
        uv run python -c "
        try:
            from src.spatial_basics import load_spatial_dataset, explore_spatial_properties, validate_spatial_data, standardize_crs
            print('âœ… All required functions are defined')

            # Check if functions are implemented (not just pass statements)
            import inspect
            functions = [load_spatial_dataset, explore_spatial_properties, validate_spatial_data, standardize_crs]
            implemented_count = 0

            for func in functions:
                source = inspect.getsource(func)
                if 'pass' not in source or len(source.strip().split('\n')) > 10:
                    implemented_count += 1
                    print(f'âœ… {func.__name__} appears to be implemented')
                else:
                    print(f'âš ï¸  {func.__name__} may not be fully implemented')

            print(f'ðŸ“ˆ Implementation progress: {implemented_count}/4 functions')
        except ImportError as e:
            print(f'âŒ Import error: {e}')
            exit(1)
        except Exception as e:
            print(f'âš ï¸  Warning during import test: {e}')
        "

    - name: ðŸ§ª Run Test Suite
      id: run_tests
      continue-on-error: true
      run: |
        echo "ðŸ§ª Running comprehensive test suite..."

        # Run tests with detailed output
        uv run pytest tests/ -v --tb=short --junit-xml=test-results.xml \
          --html=test-report.html --self-contained-html \
          2>&1 | tee test-output.log

        # Store exit code for grading
        echo "PYTEST_EXIT_CODE=$?" >> $GITHUB_ENV

    - name: ðŸ“Š Calculate Grade
      id: calculate_grade
      run: |
        echo "ðŸ“Š Calculating assignment grade..."

        # Create grading script
        cat > calculate_grade.py << 'EOF'
        import json
        import xml.etree.ElementTree as ET
        from pathlib import Path
        import os

        def calculate_grade():
            """Calculate grade based on test results."""

            # Grade breakdown (15 points total)
            function_points = {
                'load_spatial_dataset': 4,
                'explore_spatial_properties': 4,
                'validate_spatial_data': 4,
                'standardize_crs': 3
            }

            total_possible = sum(function_points.values())
            earned_points = 0
            category_breakdown = {}
            feedback = []

            # Parse test results if available
            if Path('test-results.xml').exists():
                try:
                    tree = ET.parse('test-results.xml')
                    root = tree.getroot()

                    # Count test results by function
                    for function_name, max_points in function_points.items():
                        function_tests = root.findall(f".//testcase[contains(@name, '{function_name}')]")
                        passed_tests = len([test for test in function_tests if test.find('failure') is None and test.find('error') is None])
                        total_tests = len(function_tests)

                        if total_tests > 0:
                            percentage = passed_tests / total_tests
                            earned = round(max_points * percentage)
                        else:
                            percentage = 0
                            earned = 0

                        earned_points += earned
                        category_breakdown[function_name] = {
                            'earned': earned,
                            'possible': max_points,
                            'percentage': round(percentage * 100, 1),
                            'tests_passed': passed_tests,
                            'tests_total': total_tests
                        }

                        if earned == max_points:
                            feedback.append(f"âœ… {function_name}: Excellent work! All tests passed.")
                        elif earned > 0:
                            feedback.append(f"âš ï¸  {function_name}: Partial credit. {passed_tests}/{total_tests} tests passed.")
                        else:
                            feedback.append(f"âŒ {function_name}: Not implemented or major issues. See test output for details.")

                except Exception as e:
                    feedback.append(f"âš ï¸  Error parsing test results: {e}")

            else:
                # No test results file - likely tests couldn't run
                feedback.append("âŒ Tests could not run. Check for syntax errors or missing dependencies.")
                for function_name in function_points.keys():
                    category_breakdown[function_name] = {
                        'earned': 0,
                        'possible': function_points[function_name],
                        'percentage': 0,
                        'tests_passed': 0,
                        'tests_total': 0
                    }

            # Calculate final grade
            percentage = (earned_points / total_possible) * 100

            if percentage >= 97: letter_grade = 'A+'
            elif percentage >= 93: letter_grade = 'A'
            elif percentage >= 90: letter_grade = 'A-'
            elif percentage >= 87: letter_grade = 'B+'
            elif percentage >= 83: letter_grade = 'B'
            elif percentage >= 80: letter_grade = 'B-'
            elif percentage >= 77: letter_grade = 'C+'
            elif percentage >= 73: letter_grade = 'C'
            elif percentage >= 70: letter_grade = 'C-'
            elif percentage >= 67: letter_grade = 'D+'
            elif percentage >= 63: letter_grade = 'D'
            elif percentage >= 60: letter_grade = 'D-'
            else: letter_grade = 'F'

            # Create grade report
            grade_report = {
                'total_points': earned_points,
                'possible_points': total_possible,
                'percentage': round(percentage, 1),
                'letter_grade': letter_grade,
                'category_breakdown': category_breakdown,
                'feedback': feedback,
                'assignment': 'Python GeoPandas Introduction',
                'timestamp': '2024-08-25T22:00:00Z'
            }

            # Save JSON report for instructors
            with open('grade-report.json', 'w') as f:
                json.dump(grade_report, f, indent=2)

            # Create student-friendly summary
            summary = f"""
        # ðŸ“Š Assignment Grade Report

        **Assignment**: Python GeoPandas Introduction - Spatial Data Fundamentals
        **Total Score**: {earned_points}/{total_possible} points ({percentage:.1f}%)
        **Letter Grade**: {letter_grade}

        ## ðŸ“ˆ Function Scores:
        """

            for func_name, breakdown in category_breakdown.items():
                summary += f"\n- **{func_name}**: {breakdown['earned']}/{breakdown['possible']} points ({breakdown['percentage']}%)"
                if breakdown['tests_total'] > 0:
                    summary += f" - {breakdown['tests_passed']}/{breakdown['tests_total']} tests passed"

            summary += "\n\n## ðŸ’¡ Feedback:\n"
            for item in feedback:
                summary += f"- {item}\n"

            if percentage < 80:
                summary += "\n## ðŸ”§ Next Steps:\n"
                summary += "- Review the test output below for specific error details\n"
                summary += "- Check the notebook tutorials for implementation guidance\n"
                summary += "- Test your functions interactively before submitting\n"
                summary += "- Focus on functions with 0 points first\n"

            summary += "\n---\n*Automated grading system - For questions, contact your instructor*"

            with open('grade-summary.md', 'w') as f:
                f.write(summary)

            # Set GitHub environment variables for workflow
            os.environ['ASSIGNMENT_SCORE'] = str(earned_points)
            os.environ['ASSIGNMENT_TOTAL'] = str(total_possible)
            os.environ['ASSIGNMENT_PERCENTAGE'] = f"{percentage:.1f}"
            os.environ['ASSIGNMENT_GRADE'] = letter_grade

            print(f"Grade calculated: {earned_points}/{total_possible} ({percentage:.1f}%) - {letter_grade}")
            return grade_report

        if __name__ == '__main__':
            calculate_grade()
        EOF

        # Run grading calculation
        uv run python calculate_grade.py

    - name: ðŸ“‹ Display Grade Summary
      run: |
        echo "ðŸ“‹ Assignment Grade Summary:"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

        if [ -f "grade-summary.md" ]; then
          cat grade-summary.md
        else
          echo "âš ï¸  Grade summary not generated"
        fi

        echo ""
        echo "ðŸ“Š Quick Stats:"
        echo "Score: ${{ env.ASSIGNMENT_SCORE }}/${{ env.ASSIGNMENT_TOTAL }}"
        echo "Percentage: ${{ env.ASSIGNMENT_PERCENTAGE }}%"
        echo "Grade: ${{ env.ASSIGNMENT_GRADE }}"

    - name: ðŸ“¤ Upload Test Results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results-and-grade
        path: |
          test-results.xml
          test-report.html
          test-output.log
          grade-report.json
          grade-summary.md
        retention-days: 30

    - name: ðŸ“Š Comment Grade on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          // Read grade summary if it exists
          let gradeSummary = '';
          try {
            gradeSummary = fs.readFileSync('grade-summary.md', 'utf8');
          } catch (e) {
            gradeSummary = 'âš ï¸  Grade summary not available. Check the workflow logs for details.';
          }

          const comment = `## ðŸ¤– Automated Grade Report

          ${gradeSummary}

          ---
          ðŸ“ **View detailed test results**: Check the "Artifacts" section in the workflow run
          ðŸ”„ **Re-run grading**: Push new commits to trigger automatic re-grading
          `;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

    - name: ðŸŽ¯ Workflow Summary
      if: always()
      run: |
        echo "ðŸŽ¯ Assignment Grading Complete!"
        echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        echo ""
        echo "ðŸ“Š Final Results:"
        echo "  Score: ${{ env.ASSIGNMENT_SCORE || 'N/A' }}/${{ env.ASSIGNMENT_TOTAL || '15' }}"
        echo "  Percentage: ${{ env.ASSIGNMENT_PERCENTAGE || 'N/A' }}%"
        echo "  Grade: ${{ env.ASSIGNMENT_GRADE || 'N/A' }}"
        echo ""
        echo "ðŸ“ Generated Files:"
        echo "  - grade-report.json (instructor data)"
        echo "  - grade-summary.md (student feedback)"
        echo "  - test-results.xml (detailed test data)"
        echo "  - test-report.html (visual test report)"
        echo ""
        echo "ðŸ’¡ Next Steps:"
        if [[ "${{ env.ASSIGNMENT_PERCENTAGE }}" < "80" ]]; then
          echo "  - Review test failures and error messages"
          echo "  - Work through the interactive notebooks for guidance"
          echo "  - Test functions individually before re-submitting"
          echo "  - Focus on implementing functions that scored 0 points"
        else
          echo "  - Great work! Your implementation is solid"
          echo "  - Review any partial credit areas for improvement"
          echo "  - Continue to the next assignment module"
        fi
        echo ""
        echo "ðŸŽ“ Remember: This is automated grading to help you learn."
        echo "   Contact your instructor for questions about the feedback."

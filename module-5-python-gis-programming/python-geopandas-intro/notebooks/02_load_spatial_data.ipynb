{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 📁 Loading Spatial Data - Mastering Data Input\n",
    "\n",
    "**GIST 604B - Python GeoPandas Introduction**  \n",
    "**Notebook 2: Load Spatial Data from Various Sources**\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "- Load spatial data from different file formats (GeoJSON, Shapefile, etc.)\n",
    "- Handle file paths using both strings and Path objects\n",
    "- Troubleshoot common loading errors and encoding issues\n",
    "- Implement robust error handling for spatial data loading\n",
    "- Understand the differences between spatial file formats\n",
    "- **Prepare to implement the `load_spatial_dataset()` function**\n",
    "\n",
    "## 🗂️ What You'll Practice\n",
    "\n",
    "This notebook directly prepares you to implement the **`load_spatial_dataset()`** function in your assignment. You'll learn:\n",
    "\n",
    "1. **File Format Detection**: How to determine what type of spatial file you're working with\n",
    "2. **Error Handling**: What can go wrong when loading spatial data and how to handle it\n",
    "3. **Path Management**: Working with file paths in a robust way\n",
    "4. **Data Validation**: Ensuring loaded data is actually valid spatial data\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Getting Started\n",
    "\n",
    "Let's start by importing the libraries we'll need and setting up our workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 Import required libraries\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🔧 Libraries imported successfully!\")\n",
    "print(f\"📦 GeoPandas version: {gpd.__version__}\")\n",
    "print(f\"🐼 Pandas version: {pd.__version__}\")\n",
    "\n",
    "# Check our data directory\n",
    "data_path = Path('../data')\n",
    "print(f\"\\n📁 Data directory exists: {data_path.exists()}\")\n",
    "if data_path.exists():\n",
    "    subdirs = [d.name for d in data_path.iterdir() if d.is_dir()]\n",
    "    print(f\"📂 Available datasets: {subdirs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic_loading",
   "metadata": {},
   "source": [
    "## 📖 Basic Spatial Data Loading\n",
    "\n",
    "The most fundamental operation in spatial analysis is loading data. GeoPandas makes this remarkably simple with the `gpd.read_file()` function.\n",
    "\n",
    "### 🎯 The Universal Loader\n",
    "\n",
    "Unlike regular pandas which has separate functions for different formats (`read_csv()`, `read_json()`, etc.), GeoPandas uses **one function for all spatial formats**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_load_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🌍 Load spatial data - it's this simple!\n",
    "\n",
    "print(\"📁 Loading different spatial formats with the same function:\\n\")\n",
    "\n",
    "# Method 1: Load GeoJSON\n",
    "cities_geojson = gpd.read_file('../data/cities/sample_cities.geojson')\n",
    "print(f\"📄 GeoJSON: Loaded {len(cities_geojson)} cities\")\n",
    "print(f\"   Columns: {list(cities_geojson.columns)}\")\n",
    "\n",
    "# Method 2: Load Shapefile\n",
    "cities_shapefile = gpd.read_file('../data/cities/world_cities.shp')\n",
    "print(f\"\\n📄 Shapefile: Loaded {len(cities_shapefile)} cities\")\n",
    "print(f\"   Columns: {list(cities_shapefile.columns)}\")\n",
    "\n",
    "print(\"\\n✨ Same function, different formats - GeoPandas handles the details!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "path_handling",
   "metadata": {},
   "source": [
    "## 🛣️ Working with File Paths\n",
    "\n",
    "Professional code needs to handle file paths robustly. Let's explore different ways to specify file paths and why the `pathlib.Path` approach is preferred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "path_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛣️ Different ways to handle file paths\n",
    "\n",
    "print(\"🔧 Path Handling Methods:\\n\")\n",
    "\n",
    "# Method 1: String paths (traditional)\n",
    "file_path_string = '../data/cities/sample_cities.geojson'\n",
    "cities_string = gpd.read_file(file_path_string)\n",
    "print(f\"📝 String path: '{file_path_string}'\")\n",
    "print(f\"   Result: {len(cities_string)} cities loaded\")\n",
    "\n",
    "# Method 2: Path objects (modern, recommended)\n",
    "file_path_object = Path('../data/cities/sample_cities.geojson')\n",
    "cities_path = gpd.read_file(file_path_object)\n",
    "print(f\"\\n🗂️ Path object: {file_path_object}\")\n",
    "print(f\"   Result: {len(cities_path)} cities loaded\")\n",
    "\n",
    "# Path object advantages\n",
    "print(f\"\\n💡 Path Object Advantages:\")\n",
    "print(f\"   📁 Absolute path: {file_path_object.absolute()}\")\n",
    "print(f\"   📄 File name: {file_path_object.name}\")\n",
    "print(f\"   📂 Parent directory: {file_path_object.parent}\")\n",
    "print(f\"   🔍 File exists: {file_path_object.exists()}\")\n",
    "print(f\"   📊 File size: {file_path_object.stat().st_size} bytes\")\n",
    "\n",
    "# Both approaches work identically\n",
    "print(f\"\\n✅ Both methods load identical data: {cities_string.equals(cities_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "format_comparison",
   "metadata": {},
   "source": [
    "## 🔍 Understanding Spatial File Formats\n",
    "\n",
    "Different spatial formats have different characteristics. Let's explore the most common ones and understand when to use each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format_comparison_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🗂️ Comparing different spatial file formats\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"📊 Spatial File Format Comparison:\\n\")\n",
    "\n",
    "# Load the same data in different formats\n",
    "formats = {\n",
    "    'GeoJSON': '../data/cities/world_cities.geojson',\n",
    "    'Shapefile': '../data/cities/world_cities.shp'\n",
    "}\n",
    "\n",
    "for format_name, file_path in formats.items():\n",
    "    if Path(file_path).exists():\n",
    "        # Load the data\n",
    "        gdf = gpd.read_file(file_path)\n",
    "        \n",
    "        # File size analysis\n",
    "        if format_name == 'Shapefile':\n",
    "            # Shapefile is multiple files\n",
    "            shp_dir = Path(file_path).parent\n",
    "            shp_files = list(shp_dir.glob('world_cities.*'))\n",
    "            total_size = sum(f.stat().st_size for f in shp_files)\n",
    "            file_count = len(shp_files)\n",
    "        else:\n",
    "            # Single file formats\n",
    "            total_size = Path(file_path).stat().st_size\n",
    "            file_count = 1\n",
    "        \n",
    "        print(f\"📄 {format_name}:\")\n",
    "        print(f\"   💾 Size: {total_size:,} bytes ({total_size/1024:.1f} KB)\")\n",
    "        print(f\"   📁 Files: {file_count}\")\n",
    "        print(f\"   🌍 Features: {len(gdf)}\")\n",
    "        print(f\"   📊 Columns: {len(gdf.columns)}\")\n",
    "        print(f\"   🗺️ CRS: {gdf.crs}\")\n",
    "        print()\n",
    "\n",
    "# Format characteristics\n",
    "print(\"🎯 When to Use Each Format:\")\n",
    "print(\"📄 GeoJSON:\")\n",
    "print(\"   ✅ Web applications, APIs, JavaScript\")\n",
    "print(\"   ✅ Human-readable text format\")\n",
    "print(\"   ✅ Single file, easy to share\")\n",
    "print(\"   ⚠️  Larger file sizes\")\n",
    "print(\"\")\n",
    "print(\"📄 Shapefile:\")\n",
    "print(\"   ✅ Traditional GIS software\")\n",
    "print(\"   ✅ Widely supported\")\n",
    "print(\"   ✅ Compact file size\")\n",
    "print(\"   ⚠️  Multiple files to manage\")\n",
    "print(\"   ⚠️  Column name limitations (10 chars)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error_handling",
   "metadata": {},
   "source": [
    "## ⚠️ Error Handling and Troubleshooting\n",
    "\n",
    "Real-world spatial data loading often involves problems. Let's explore common errors and how to handle them professionally:\n",
    "\n",
    "### 🚨 Common Loading Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚨 Demonstrating common errors and solutions\n",
    "\n",
    "print(\"🕵️ Common Spatial Data Loading Errors:\\n\")\n",
    "\n",
    "# Error 1: File doesn't exist\n",
    "print(\"❌ Error 1: File Not Found\")\n",
    "try:\n",
    "    missing_file = gpd.read_file('nonexistent_file.geojson')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"   🚨 Error: {e}\")\n",
    "    print(\"   💡 Solution: Check file path and spelling\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Error 2: Invalid format\n",
    "print(\"❌ Error 2: Invalid File Format\")\n",
    "# Create a temporary invalid file\n",
    "invalid_file = Path('../data/invalid_spatial_file.txt')\n",
    "invalid_file.write_text(\"This is not spatial data\")\n",
    "\n",
    "try:\n",
    "    invalid_data = gpd.read_file(invalid_file)\n",
    "except Exception as e:\n",
    "    print(f\"   🚨 Error type: {type(e).__name__}\")\n",
    "    print(f\"   🚨 Error message: {e}\")\n",
    "    print(\"   💡 Solution: Ensure file contains valid spatial data\")\n",
    "\n",
    "# Clean up\n",
    "invalid_file.unlink()  # Delete the temporary file\n",
    "\n",
    "print()\n",
    "\n",
    "# Error 3: Corrupted data (using our problematic dataset)\n",
    "print(\"❌ Error 3: Data Quality Issues\")\n",
    "try:\n",
    "    problematic = gpd.read_file('../data/cities/cities_with_issues.geojson')\n",
    "    print(f\"   ⚠️  File loads but has {problematic.geometry.isna().sum()} missing geometries\")\n",
    "    print(f\"   ⚠️  File loads but has {(~problematic.geometry.is_valid).sum()} invalid geometries\")\n",
    "    print(\"   💡 Solution: Load successfully, then validate and clean data\")\n",
    "except Exception as e:\n",
    "    print(f\"   🚨 Error: {e}\")\n",
    "    print(\"   💡 Solution: Check data integrity and format\")\n",
    "\n",
    "print(\"\\n✅ Key Takeaway: Always handle errors gracefully in production code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust_loading_function",
   "metadata": {},
   "source": [
    "## 🛡️ Building a Robust Loading Function\n",
    "\n",
    "Now let's build a professional-grade loading function step by step. This will guide you toward implementing the `load_spatial_dataset()` function in your assignment:\n",
    "\n",
    "### 🔧 Step-by-Step Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust_loader_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🛡️ Building a robust spatial data loader\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "def demo_load_spatial_dataset(file_path: Union[str, Path], **kwargs) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Demonstration of robust spatial data loading.\n",
    "    This shows you the approach for your assignment implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🔄 Loading spatial data from: {file_path}\")\n",
    "    \n",
    "    # Step 1: Convert to Path object\n",
    "    path_obj = Path(file_path)\n",
    "    print(f\"   📁 Using path object: {path_obj}\")\n",
    "    \n",
    "    # Step 2: Check if file exists\n",
    "    if not path_obj.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path_obj}\")\n",
    "    \n",
    "    print(f\"   ✅ File exists: {path_obj.stat().st_size} bytes\")\n",
    "    \n",
    "    # Step 3: Determine file format\n",
    "    file_extension = path_obj.suffix.lower()\n",
    "    print(f\"   🗂️ File format: {file_extension}\")\n",
    "    \n",
    "    # Step 4: Validate supported format\n",
    "    supported_formats = ['.geojson', '.json', '.shp', '.gpkg']\n",
    "    if file_extension not in supported_formats:\n",
    "        raise ValueError(f\"Unsupported format: {file_extension}. Supported: {supported_formats}\")\n",
    "    \n",
    "    # Step 5: Load the data\n",
    "    try:\n",
    "        gdf = gpd.read_file(path_obj, **kwargs)\n",
    "        print(f\"   📊 Loaded {len(gdf)} features\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading spatial data: {e}\")\n",
    "    \n",
    "    # Step 6: Basic validation\n",
    "    if not isinstance(gdf, gpd.GeoDataFrame):\n",
    "        raise ValueError(\"Loaded data is not a valid GeoDataFrame\")\n",
    "    \n",
    "    if len(gdf) == 0:\n",
    "        print(\"   ⚠️ Warning: Dataset is empty\")\n",
    "    \n",
    "    if 'geometry' not in gdf.columns:\n",
    "        raise ValueError(\"No geometry column found in the data\")\n",
    "    \n",
    "    print(f\"   ✅ Validation passed: {len(gdf)} features with geometry\")\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "# Test the function\n",
    "print(\"🧪 Testing robust loading function:\\n\")\n",
    "\n",
    "# Test 1: Valid GeoJSON\n",
    "try:\n",
    "    cities = demo_load_spatial_dataset('../data/cities/sample_cities.geojson')\n",
    "    print(f\"   Success! Loaded {len(cities)} cities\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\\n\")\n",
    "\n",
    "# Test 2: Valid Shapefile\n",
    "try:\n",
    "    cities_shp = demo_load_spatial_dataset('../data/cities/world_cities.shp')\n",
    "    print(f\"   Success! Loaded {len(cities_shp)} cities\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\\n\")\n",
    "\n",
    "# Test 3: File doesn't exist\n",
    "try:\n",
    "    missing = demo_load_spatial_dataset('missing_file.geojson')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"   Expected error caught: {e}\\n\")\n",
    "\n",
    "print(\"🎯 This demonstrates the approach for your assignment implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loading_scenarios",
   "metadata": {},
   "source": [
    "## 🌐 Advanced Loading Scenarios\n",
    "\n",
    "Let's explore more advanced loading scenarios you might encounter in real-world projects:\n",
    "\n",
    "### 📊 Loading with Additional Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Advanced loading with parameters\n",
    "\n",
    "print(\"🌐 Advanced Loading Scenarios:\\n\")\n",
    "\n",
    "# Scenario 1: Load with encoding specification\n",
    "print(\"📝 Scenario 1: Explicit encoding\")\n",
    "try:\n",
    "    cities_utf8 = gpd.read_file('../data/cities/sample_cities.geojson', encoding='utf-8')\n",
    "    print(f\"   ✅ Loaded with UTF-8 encoding: {len(cities_utf8)} cities\")\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️ Error: {e}\")\n",
    "\n",
    "# Scenario 2: Load specific columns (for Shapefiles)\n",
    "print(\"\\n📊 Scenario 2: Load specific columns\")\n",
    "try:\n",
    "    # For demonstration, load only specific columns\n",
    "    cities_subset = gpd.read_file('../data/cities/world_cities.shp', \n",
    "                                 columns=['name', 'country', 'geometry'])\n",
    "    print(f\"   ✅ Loaded subset: {len(cities_subset)} cities with {len(cities_subset.columns)} columns\")\n",
    "    print(f\"   📋 Columns: {list(cities_subset.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️ Error: {e}\")\n",
    "\n",
    "# Scenario 3: Load with bounding box filter (if supported)\n",
    "print(\"\\n🗺️ Scenario 3: Spatial filtering (bbox)\")\n",
    "try:\n",
    "    # Define a bounding box (minx, miny, maxx, maxy) for North America\n",
    "    north_america_bbox = [-130, 25, -60, 50]\n",
    "    cities_na = gpd.read_file('../data/cities/world_cities.geojson', \n",
    "                             bbox=north_america_bbox)\n",
    "    print(f\"   ✅ Loaded North America cities: {len(cities_na)} cities\")\n",
    "    if len(cities_na) > 0:\n",
    "        print(f\"   🌍 Sample cities: {', '.join(cities_na['name'].head(3))}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ⚠️ Bbox filtering not supported for this format: {e}\")\n",
    "\n",
    "print(\"\\n💡 Key Insight: The **kwargs parameter allows flexible loading options!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_validation",
   "metadata": {},
   "source": [
    "## ✅ Data Validation After Loading\n",
    "\n",
    "Loading data is just the first step. Professional spatial analysis always includes data validation:\n",
    "\n",
    "### 🔍 Essential Validation Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Post-loading data validation\n",
    "\n",
    "def validate_loaded_data(gdf: gpd.GeoDataFrame, data_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Demonstrate validation checks after loading spatial data.\n",
    "    Returns a validation report.\n",
    "    \"\"\"\n",
    "    print(f\"🔍 Validating loaded data: {data_name}\")\n",
    "    \n",
    "    validation_report = {\n",
    "        'is_valid': True,\n",
    "        'issues': [],\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    # Check 1: Is it actually a GeoDataFrame?\n",
    "    if not isinstance(gdf, gpd.GeoDataFrame):\n",
    "        validation_report['is_valid'] = False\n",
    "        validation_report['issues'].append(\"Not a GeoDataFrame\")\n",
    "        return validation_report\n",
    "    \n",
    "    print(f\"   ✅ Type: Valid GeoDataFrame\")\n",
    "    \n",
    "    # Check 2: Has data?\n",
    "    if len(gdf) == 0:\n",
    "        validation_report['warnings'].append(\"Dataset is empty\")\n",
    "        print(f\"   ⚠️ Data: Empty dataset\")\n",
    "    else:\n",
    "        print(f\"   ✅ Data: {len(gdf)} features\")\n",
    "    \n",
    "    # Check 3: Has geometry column?\n",
    "    if 'geometry' not in gdf.columns:\n",
    "        validation_report['is_valid'] = False\n",
    "        validation_report['issues'].append(\"No geometry column\")\n",
    "        print(f\"   ❌ Geometry: Missing geometry column\")\n",
    "    else:\n",
    "        print(f\"   ✅ Geometry: Column present\")\n",
    "        \n",
    "        # Check geometry validity\n",
    "        if len(gdf) > 0:\n",
    "            valid_geoms = gdf.geometry.is_valid.sum()\n",
    "            total_geoms = len(gdf)\n",
    "            print(f\"   📊 Valid geometries: {valid_geoms}/{total_geoms}\")\n",
    "            \n",
    "            if valid_geoms < total_geoms:\n",
    "                validation_report['warnings'].append(f\"{total_geoms - valid_geoms} invalid geometries\")\n",
    "    \n",
    "    # Check 4: Has CRS?\n",
    "    if gdf.crs is None:\n",
    "        validation_report['warnings'].append(\"No coordinate reference system defined\")\n",
    "        print(f\"   ⚠️ CRS: Not defined\")\n",
    "    else:\n",
    "        print(f\"   ✅ CRS: {gdf.crs}\")\n",
    "    \n",
    "    # Check 5: Reasonable data ranges (for geographic data)\n",
    "    if gdf.crs and gdf.crs.to_epsg() == 4326 and len(gdf) > 0:  # WGS84\n",
    "        bounds = gdf.total_bounds\n",
    "        minx, miny, maxx, maxy = bounds\n",
    "        \n",
    "        if not (-180 <= minx <= 180 and -180 <= maxx <= 180):\n",
    "            validation_report['warnings'].append(\"Longitude values outside valid range\")\n",
    "            print(f\"   ⚠️ Longitude range: {minx:.2f} to {maxx:.2f} (outside ±180)\")\n",
    "        else:\n",
    "            print(f\"   ✅ Longitude range: {minx:.2f} to {maxx:.2f}\")\n",
    "            \n",
    "        if not (-90 <= miny <= 90 and -90 <= maxy <= 90):\n",
    "            validation_report['warnings'].append(\"Latitude values outside valid range\")\n",
    "            print(f\"   ⚠️ Latitude range: {miny:.2f} to {maxy:.2f} (outside ±90)\")\n",
    "        else:\n",
    "            print(f\"   ✅ Latitude range: {miny:.2f} to {maxy:.2f}\")\n",
    "    \n",
    "    # Summary\n",
    "    if validation_report['is_valid']:\n",
    "        if len(validation_report['warnings']) == 0:\n",
    "            print(f\"   🎉 Overall: EXCELLENT - No issues found!\")\n",
    "        else:\n",
    "            print(f\"   ✅ Overall: GOOD - {len(validation_report['warnings'])} warnings\")\n",
    "    else:\n",
    "        print(f\"   ❌ Overall: PROBLEMS - {len(validation_report['issues'])} critical issues\")\n",
    "    \n",
    "    return validation_report\n",
    "\n",
    "# Test validation on different datasets\n",
    "print(\"🧪 Testing data validation:\\n\")\n",
    "\n",
    "# Test 1: Clean data\n",
    "cities_clean = gpd.read_file('../data/cities/sample_cities.geojson')\n",
    "report1 = validate_loaded_data(cities_clean, \"Clean Cities\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Test 2: Problematic data\n",
    "try:\n",
    "    cities_problems = gpd.read_file('../data/cities/cities_with_issues.geojson')\n",
    "    report2 = validate_loaded_data(cities_problems, \"Problematic Cities\")\n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Failed to load problematic data: {e}\")\n",
    "\n",
    "print(\"\\n💡 Validation helps identify data quality issues early in your workflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assignment_preparation",
   "metadata": {},
   "source": [
    "## 🎯 Assignment Implementation Guide\n",
    "\n",
    "Now you're ready to implement the `load_spatial_dataset()` function! Here's your roadmap:\n",
    "\n",
    "### 📋 Implementation Checklist\n",
    "\n",
    "Your function needs to:\n",
    "\n",
    "1. ✅ **Accept flexible input**: `Union[str, Path]` for file paths\n",
    "2. ✅ **Handle Path conversion**: Convert strings to Path objects\n",
    "3. ✅ **Check file existence**: Raise `FileNotFoundError` if missing\n",
    "4. ✅ **Validate file format**: Support common spatial formats\n",
    "5. ✅ **Load the data**: Use `gpd.read_file()` with error handling\n",
    "6. ✅ **Validate results**: Ensure it's a valid GeoDataFrame\n",
    "7. ✅ **Handle kwargs**: Pass additional parameters to GeoPandas\n",
    "8. ✅ **Return GeoDataFrame**: Return the loaded spatial data\n",
    "\n",
    "### 💻 Code Structure Template\n",
    "\n",
    "```python\n",
    "def load_spatial_dataset(file_path: Union[str, Path], **kwargs) -> gpd.GeoDataFrame:\n",
    "    # Step 1: Convert to Path object\n",
    "    path_obj = Path(file_path)\n",
    "    \n",
    "    # Step 2: Check existence\n",
    "    if not path_obj.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path_obj}\")\n",
    "    \n",
    "    # Step 3: Validate format (optional but recommended)\n",
    "    # Check file extension\n",
    "    \n",
    "    # Step 4: Load data with error handling\n",
    "    try:\n",
    "        gdf = gpd.read_file(path_obj, **kwargs)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error loading spatial data: {e}\")\n",
    "    \n",
    "    # Step 5: Validate result\n",
    "    # Check it's a GeoDataFrame, has geometry, etc.\n",
    "    \n",
    "    # Step 6: Return the data\n",
    "    return gdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "testing_your_function",
   "metadata": {},
   "source": [
    "## 🧪 Testing Your Implementation\n",
    "\n",
    "After implementing your function, test it with these scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_scenarios",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 Test scenarios for your load_spatial_dataset() function\n",
    "# Uncomment these after implementing your function!\n",
    "\n",
    "# from src.spatial_basics import load_spatial_dataset\n",
    "\n",
    "print(\"🧪 Test Scenarios for load_spatial_dataset()\\n\")\n",
    "\n",
    "# Test cases you should handle:\n",
    "test_cases = [\n",
    "    \"✅ Valid GeoJSON file with string path\",\n",
    "    \"✅ Valid Shapefile with Path object\", \n",
    "    \"✅ Loading with additional kwargs (encoding)\",\n",
    "    \"❌ Non-existent file (should raise FileNotFoundError)\",\n",
    "    \"❌ Invalid file format (should raise ValueError)\",\n",
    "    \"⚠️ File with data quality issues (should load but warn)\"\n",
    "]\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"{i}. {test_case}\")\n",
    "\n",
    "print(\"\\n💡 Run: uv run pytest tests/ -k 'load_spatial_dataset' -v\")\n",
    "print(\"   This will test your implementation automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key_concepts",
   "metadata": {},
   "source": [
    "## 🎓 Key Concepts Summary\n",
    "\n",
    "### ✅ What You've Learned:\n",
    "\n",
    "**📁 File Path Handling**\n",
    "- Using `pathlib.Path` for robust path management\n",
    "- Converting between strings and Path objects\n",
    "- Checking file existence before loading\n",
    "\n",
    "**🗂️ Spatial Data Formats**\n",
    "- GeoJSON: Web-friendly, single file, human-readable\n",
    "- Shapefile: Traditional GIS, multiple files, compact\n",
    "- Understanding when to use each format\n",
    "\n",
    "**⚠️ Error Handling**\n",
    "- `FileNotFoundError` for missing files\n",
    "- `ValueError` for invalid data or formats\n",
    "- Using try/except blocks for graceful failure\n",
    "\n",
    "**✅ Data Validation**\n",
    "- Checking data type (GeoDataFrame)\n",
    "- Verifying geometry column exists\n",
    "- Validating coordinate ranges\n",
    "- CRS presence and validity\n",
    "\n",
    "**🔧 Professional Practices**\n",
    "- Using type hints (`Union[str, Path]`)\n",
    "- Supporting flexible parameters (`**kwargs`)\n",
    "- Comprehensive error messages\n",
    "- Step-by-step validation workflow\n",
    "\n",
    "### 🚀 Skills for Your GIS Career:\n",
    "- **Data Pipeline Development**: Building robust data loading functions\n",
    "- **Error Handling**: Making code that works with messy real-world data  \n",
    "- **Format Expertise**: Understanding spatial data format trade-offs\n",
    "- **Quality Assurance**: Validating data immediately after loading\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Next Steps\n",
    "\n",
    "### 🎯 Immediate Actions:\n",
    "1. **Implement** `load_spatial_dataset()` in `src/spatial_basics.py`\n",
    "2. **Test** with: `uv run pytest tests/ -k \"load_spatial_dataset\" -v`\n",
    "3. **Debug** any failing tests using the error messages\n",
    "4. **Validate** with different file formats and scenarios\n",
    "\n",
    "### 📖 Continue Learning:\n",
    "- **Next Notebook**: `03_explore_properties.ipynb` - Analyze spatial characteristics\n",
    "- **Build on**: This loading foundation for all future spatial analysis\n",
    "- **Practice**: Try loading your own spatial datasets!\n",
    "\n",
    "### 💡 Pro Tips:\n",
    "- **Always validate** loaded data before analysis\n",
    "- **Handle errors gracefully** - real data is messy\n",
    "- **Use Path objects** - they prevent many path-related bugs\n",
    "- **Test edge cases** - empty files, missing files, wrong formats\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Congratulations!** You now understand professional spatial data loading. This is the foundation skill that supports all GIS programming work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick_reference_loading",
   "metadata": {},
   "source": [
    "## 🔖 Quick Reference - Loading Commands\n",
    "\n",
    "```python\n",
    "# Basic loading\n",
    "gdf = gpd.read_file('data/file.geojson')\n",
    "gdf = gpd.read_file('data/file.shp')\n",
    "\n",
    "# With Path objects (recommended)\n",
    "from pathlib import Path\n",
    "file_path = Path('data/file.geojson')\n",
    "gdf = gpd.read_file(file_path)\n",
    "\n",
    "# With additional parameters\n",
    "gdf = gpd.read_file('file.geojson', encoding='utf-8')\n",
    "gdf = gpd.read_file('file.shp', columns=['name', 'geometry'])\n",
    "\n",
    "# Error handling\n",
    "try:\n",
    "    gdf = gpd.read_file('file.geojson')\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")\n",
    "except Exception as e:\n",
    "    print(f\"Loading error: {e}\")\n",
    "\n",
    "# Basic validation\n",
    "assert isinstance(gdf, gpd.GeoDataFrame)\n",
    "assert 'geometry' in gdf.columns\n",
    "assert len(gdf) > 0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

name: GeoPandas Spatial Analysis - Automated Grading Pipeline

on:
  push:
    branches: [ main, geopandas-assignment, assignment-* ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual trigger

env:
  PYTHON_VERSION: "3.13"
  UV_VERSION: "0.1.18"

jobs:
  spatial-analysis-assessment:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    strategy:
      matrix:
        python-version: ["3.13"]
        os: [ubuntu-latest]
      fail-fast: false

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - name: Install System Spatial Libraries
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          gdal-bin \
          libgdal-dev \
          libgeos-dev \
          libproj-dev \
          libspatialindex-dev

        # Set environment variables for spatial libraries
        export CPLUS_INCLUDE_PATH=/usr/include/gdal
        export C_INCLUDE_PATH=/usr/include/gdal

    - name: Install uv Package Manager
      uses: astral-sh/setup-uv@v1
      with:
        version: ${{ env.UV_VERSION }}
        enable-cache: true

    - name: Setup Python Environment
      run: |
        uv python install ${{ matrix.python-version }}
        uv python pin ${{ matrix.python-version }}

    - name: Create Virtual Environment
      run: |
        uv venv --python ${{ matrix.python-version }}
        echo "VIRTUAL_ENV=.venv" >> $GITHUB_ENV
        echo "$PWD/.venv/bin" >> $GITHUB_PATH

    - name: Install Dependencies
      run: |
        uv pip install --upgrade pip
        uv sync --all-extras --dev
      timeout-minutes: 8

    - name: Verify Spatial Installation
      run: |
        uv pip list
        python --version
        which python

        # Test spatial libraries
        python -c "import geopandas as gpd; print(f'GeoPandas: {gpd.__version__}')"
        python -c "import shapely; print(f'Shapely: {shapely.__version__}')"
        python -c "import fiona; print(f'Fiona: {fiona.__version__}')"
        python -c "import pyproj; print(f'PyProj: {pyproj.__version__}')"

        # Test basic spatial functionality
        python -c "
import geopandas as gpd
from shapely.geometry import Point
test_gdf = gpd.GeoDataFrame({'geometry': [Point(0, 0)]}, crs='EPSG:4326')
print('‚úÖ Spatial environment ready!')
"

    - name: Generate Sample Data
      run: |
        echo "Generating sample spatial data for tests..."
        python data/create_sample_data.py

    - name: Code Quality - Black Formatting Check
      id: black-check
      run: |
        echo "Running Black formatting check for spatial analysis code..."
        uv run black --check --diff src/ tests/ || {
          echo "black-failed=true" >> $GITHUB_OUTPUT
          echo "‚ùå Code formatting issues found. Run 'black src/ tests/' to fix."
          exit 1
        }
        echo "‚úÖ Spatial code formatting passed"
      continue-on-error: true

    - name: Code Quality - Ruff Linting
      id: ruff-check
      run: |
        echo "Running Ruff linting for spatial analysis code..."
        uv run ruff check src/ tests/ --output-format=github || {
          echo "ruff-failed=true" >> $GITHUB_OUTPUT
          echo "‚ùå Spatial code linting issues found."
          exit 1
        }
        echo "‚úÖ Spatial code linting passed"
      continue-on-error: true

    - name: Code Quality - MyPy Type Checking
      id: mypy-check
      run: |
        echo "Running MyPy type checking for spatial analysis code..."
        uv run mypy src/ --no-error-summary || {
          echo "mypy-failed=true" >> $GITHUB_OUTPUT
          echo "‚ùå Spatial code type checking issues found."
          exit 1
        }
        echo "‚úÖ Spatial code type checking passed"
      continue-on-error: true

    - name: Security Scan - Bandit
      id: bandit-check
      run: |
        echo "Running Bandit security scan..."
        uv run bandit -r src/ -f json -o bandit-report.json || {
          echo "bandit-failed=true" >> $GITHUB_OUTPUT
          echo "‚ö†Ô∏è  Security issues found"
          exit 1
        }
        echo "‚úÖ Security scan passed"
      continue-on-error: true

    - name: Run Spatial Analysis Unit Tests
      id: pytest-run
      run: |
        echo "Running spatial analysis tests with coverage..."
        uv run pytest tests/ \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term \
          --cov-fail-under=60 \
          --junitxml=test-results.xml \
          --tb=short \
          -v \
          --durations=10 \
          -m "not slow" || {
            echo "pytest-failed=true" >> $GITHUB_OUTPUT
            echo "‚ùå Some spatial analysis tests failed"
        }

        # Extract test statistics
        if [ -f test-results.xml ]; then
          python -c "
import xml.etree.ElementTree as ET
try:
    tree = ET.parse('test-results.xml')
    root = tree.getroot()
    tests = root.get('tests', '0')
    failures = root.get('failures', '0')
    errors = root.get('errors', '0')
    passed = int(tests) - int(failures) - int(errors)
    print(f'üó∫Ô∏è  Spatial Tests: {tests}, Passed: {passed}, Failed: {failures}, Errors: {errors}')
    with open('$GITHUB_OUTPUT', 'a') as f:
        f.write(f'tests-total={tests}\n')
        f.write(f'tests-passed={passed}\n')
        f.write(f'tests-failed={failures}\n')
        f.write(f'tests-errors={errors}\n')
except Exception as e:
    print(f'Error parsing spatial test results: {e}')
"
        fi
      continue-on-error: true

    - name: Run Spatial Performance Benchmarks
      id: benchmark-run
      run: |
        echo "Running spatial analysis performance benchmarks..."
        uv run pytest benchmarks/ \
          --benchmark-only \
          --benchmark-json=benchmark.json \
          --benchmark-min-rounds=3 \
          --benchmark-max-time=30 || {
            echo "benchmark-failed=true" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è  Spatial performance benchmarks failed"
        }

        if [ -f benchmark.json ]; then
          echo "‚úÖ Spatial performance benchmarks completed"
          python -c "
import json
try:
    with open('benchmark.json', 'r') as f:
        data = json.load(f)
        benchmarks = data.get('benchmarks', [])
        print(f'üìä Completed {len(benchmarks)} spatial performance benchmarks')
        with open('$GITHUB_OUTPUT', 'a') as f:
            f.write(f'benchmark-count={len(benchmarks)}\n')
except Exception as e:
    print(f'Error reading spatial benchmark results: {e}')
"
        fi
      continue-on-error: true

    - name: Calculate Final Grade
      if: always()
      run: |
        echo "Calculating automated grade for spatial analysis assignment..."

        # Extract results from previous steps
        TESTS_TOTAL="${{ steps.pytest-run.outputs.tests-total || 0 }}"
        TESTS_PASSED="${{ steps.pytest-run.outputs.tests-passed || 0 }}"
        TESTS_FAILED="${{ steps.pytest-run.outputs.tests-failed || 0 }}"

        BLACK_FAILED="${{ steps.black-check.outputs.black-failed || false }}"
        RUFF_FAILED="${{ steps.ruff-check.outputs.ruff-failed || false }}"
        MYPY_FAILED="${{ steps.mypy-check.outputs.mypy-failed || false }}"
        BANDIT_FAILED="${{ steps.bandit-check.outputs.bandit-failed || false }}"
        PYTEST_FAILED="${{ steps.pytest-run.outputs.pytest-failed || false }}"
        BENCHMARK_FAILED="${{ steps.benchmark-run.outputs.benchmark-failed || false }}"

        python << 'EOF'
import os
import json

def calculate_grade():
    """Calculate automated grade for GeoPandas spatial analysis assignment (30 points total)"""

    # Get environment variables
    tests_total = int(os.getenv('TESTS_TOTAL', '0'))
    tests_passed = int(os.getenv('TESTS_PASSED', '0'))
    tests_failed = int(os.getenv('TESTS_FAILED', '0'))

    # Quality check failures
    black_failed = os.getenv('BLACK_FAILED', 'false').lower() == 'true'
    ruff_failed = os.getenv('RUFF_FAILED', 'false').lower() == 'true'
    mypy_failed = os.getenv('MYPY_FAILED', 'false').lower() == 'true'
    bandit_failed = os.getenv('BANDIT_FAILED', 'false').lower() == 'true'
    pytest_failed = os.getenv('PYTEST_FAILED', 'false').lower() == 'true'
    benchmark_failed = os.getenv('BENCHMARK_FAILED', 'false').lower() == 'true'

    print("=== GeoPandas Spatial Analysis Assignment - Automated Grading ===")
    print(f"üìä Total Tests: {tests_total}")
    print(f"‚úÖ Tests Passed: {tests_passed}")
    print(f"‚ùå Tests Failed: {tests_failed}")

    # Grade breakdown (30 points total)
    # - Core functionality tests: 20 points (weighted by test passage)
    # - Code quality: 5 points
    # - Performance: 3 points
    # - Documentation/Style: 2 points

    # Core functionality (20 points)
    if tests_total > 0:
        test_score = (tests_passed / tests_total) * 20
    else:
        test_score = 0

    # Code quality (5 points)
    quality_deductions = 0
    if black_failed:
        quality_deductions += 1.5
        print("‚ùå Code formatting issues (-1.5 points)")
    if ruff_failed:
        quality_deductions += 1.5
        print("‚ùå Code linting issues (-1.5 points)")
    if mypy_failed:
        quality_deductions += 1
        print("‚ùå Type checking issues (-1.0 points)")
    if bandit_failed:
        quality_deductions += 1
        print("‚ùå Security issues found (-1.0 points)")

    quality_score = max(0, 5 - quality_deductions)

    # Performance (3 points)
    if benchmark_failed:
        performance_score = 1  # Partial credit
        print("‚ö†Ô∏è  Performance benchmark issues (-2 points)")
    else:
        performance_score = 3
        print("‚úÖ Performance benchmarks passed")

    # Documentation/Style (2 points) - based on code quality
    if black_failed or ruff_failed:
        style_score = 0
    else:
        style_score = 2

    # Calculate total
    total_score = test_score + quality_score + performance_score + style_score
    total_score = min(30, max(0, total_score))  # Cap between 0-30

    print("\n=== GRADE BREAKDOWN ===")
    print(f"üìù Core Functionality: {test_score:.1f}/20 points")
    print(f"üîç Code Quality: {quality_score:.1f}/5 points")
    print(f"‚ö° Performance: {performance_score:.1f}/3 points")
    print(f"üìñ Style/Documentation: {style_score:.1f}/2 points")
    print(f"üìä TOTAL SCORE: {total_score:.1f}/30 points ({total_score/30*100:.1f}%)")

    # Letter grade
    if total_score >= 27:
        letter_grade = "A"
    elif total_score >= 24:
        letter_grade = "A-"
    elif total_score >= 21:
        letter_grade = "B+"
    elif total_score >= 18:
        letter_grade = "B"
    elif total_score >= 15:
        letter_grade = "B-"
    elif total_score >= 12:
        letter_grade = "C"
    else:
        letter_grade = "F"

    print(f"üéì LETTER GRADE: {letter_grade}")

    # Create summary for GitHub
    summary = {
        'total_score': round(total_score, 1),
        'max_score': 30,
        'percentage': round(total_score/30*100, 1),
        'letter_grade': letter_grade,
        'tests_passed': tests_passed,
        'tests_total': tests_total,
        'breakdown': {
            'functionality': round(test_score, 1),
            'quality': round(quality_score, 1),
            'performance': round(performance_score, 1),
            'style': round(style_score, 1)
        }
    }

    # Save to GitHub outputs
    with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
        f.write(f'final-score={total_score:.1f}\n')
        f.write(f'letter-grade={letter_grade}\n')
        f.write(f'percentage={total_score/30*100:.1f}\n')

    # Save detailed results
    with open('grading-results.json', 'w') as f:
        json.dump(summary, f, indent=2)

    return total_score

if __name__ == '__main__':
    calculate_grade()
EOF

    - name: Upload Test Reports
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: spatial-analysis-test-reports
        path: |
          test-results.xml
          htmlcov/
          benchmark.json
          bandit-report.json
          grading-results.json

    - name: Comment Grade on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          try {
            const results = JSON.parse(fs.readFileSync('grading-results.json', 'utf8'));

            const comment = `
## üó∫Ô∏è GeoPandas Spatial Analysis Assignment - Automated Grade

**Final Score: ${results.total_score}/${results.max_score} points (${results.percentage}%)**
**Letter Grade: ${results.letter_grade}**

### Grade Breakdown:
- üìù **Core Functionality**: ${results.breakdown.functionality}/20 points
- üîç **Code Quality**: ${results.breakdown.quality}/5 points
- ‚ö° **Performance**: ${results.breakdown.performance}/3 points
- üìñ **Style/Documentation**: ${results.breakdown.style}/2 points

### Test Results:
- ‚úÖ Tests Passed: ${results.tests_passed}/${results.tests_total}

*This is an automated assessment. Your instructor may provide additional feedback.*
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('Could not post grade comment:', error);
          }

    - name: Final Status
      if: always()
      run: |
        echo "üéØ GeoPandas Spatial Analysis Assignment Assessment Complete"
        echo "üìä Final Score: ${{ steps.calculate-grade.outputs.final-score || 'N/A' }}/30 points"
        echo "üéì Letter Grade: ${{ steps.calculate-grade.outputs.letter-grade || 'N/A' }}"
        echo ""
        echo "For detailed results, check the uploaded artifacts and grading-results.json"

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function 4: Join Station Data ğŸ”—\n",
    "\n",
    "**Welcome to data joining with pandas!**\n",
    "\n",
    "In this notebook, you'll learn how to build the `join_station_data()` function step by step. This is like connecting two Excel spreadsheets that share a common column - in our case, combining weather station locations with temperature readings.\n",
    "\n",
    "## ğŸ¯ What This Function Does\n",
    "- Combines two separate datasets using a common column (station_id)\n",
    "- Adds location information (station name, coordinates) to temperature readings\n",
    "- Handles cases where stations might not have readings or readings might not have stations\n",
    "- Reports information about the join operation\n",
    "- Returns a combined DataFrame with all relevant information\n",
    "\n",
    "## ğŸ”§ Function Signature\n",
    "```python\n",
    "def join_station_data(stations_df, readings_df):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        stations_df (pandas.DataFrame): Weather station information with columns:\n",
    "            - station_id: Unique station identifier\n",
    "            - station_name: Human-readable station name\n",
    "            - latitude, longitude: Station coordinates\n",
    "            - elevation_m: Station elevation\n",
    "            \n",
    "        readings_df (pandas.DataFrame): Environmental measurements with columns:\n",
    "            - station_id: Links to stations_df\n",
    "            - date: Measurement date\n",
    "            - temperature_c: Temperature measurement\n",
    "            - humidity_percent: Humidity measurement\n",
    "            - data_quality: Quality flag\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Combined data with both station info and readings\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 1: Import Libraries and Load Both Datasets\n",
    "\n",
    "First, let's set up our environment and load both the station and readings data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(f\"âœ… Pandas version: {pd.__version__}\")\n",
    "print(\"ğŸ”— Ready to join datasets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both datasets\n",
    "stations_file = '../data/weather_stations.csv'\n",
    "readings_file = '../data/temperature_readings.csv'\n",
    "\n",
    "stations_df = pd.read_csv(stations_file)\n",
    "readings_df = pd.read_csv(readings_file)\n",
    "\n",
    "print(f\"ğŸª Loaded {len(stations_df)} weather stations\")\n",
    "print(f\"ğŸ“Š Loaded {len(readings_df)} temperature readings\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Station columns: {list(stations_df.columns)}\")\n",
    "print(f\"ğŸ“‹ Readings columns: {list(readings_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Step 2: Understanding Your Datasets\n",
    "\n",
    "Before joining, let's examine both datasets to understand their structure and relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the stations dataset\n",
    "print(\"ğŸª WEATHER STATIONS DATASET:\\n\")\n",
    "print(f\"Shape: {stations_df.shape}\")\n",
    "print(\"\\nğŸ” First 3 stations:\")\n",
    "display(stations_df.head(3))\n",
    "\n",
    "print(\"\\nğŸ“Š Station IDs in stations dataset:\")\n",
    "print(sorted(stations_df['station_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the readings dataset\n",
    "print(\"ğŸŒ¡ï¸  TEMPERATURE READINGS DATASET:\\n\")\n",
    "print(f\"Shape: {readings_df.shape}\")\n",
    "print(\"\\nğŸ” First 3 readings:\")\n",
    "display(readings_df.head(3))\n",
    "\n",
    "print(\"\\nğŸ“Š Station IDs in readings dataset:\")\n",
    "print(sorted(readings_df['station_id'].unique()))\n",
    "\n",
    "print(\"\\nğŸ“ˆ Readings per station:\")\n",
    "readings_count = readings_df['station_id'].value_counts().sort_index()\n",
    "for station, count in readings_count.items():\n",
    "    print(f\"   {station}: {count} readings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— Step 3: Understanding the Common Key\n",
    "\n",
    "The key to joining datasets is identifying the common column(s) that link them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the common key: station_id\n",
    "print(\"ğŸ”‘ ANALYZING THE COMMON KEY: station_id\\n\")\n",
    "\n",
    "stations_ids = set(stations_df['station_id'].unique())\n",
    "readings_ids = set(readings_df['station_id'].unique())\n",
    "\n",
    "print(f\"ğŸ“Š Stations with location data: {len(stations_ids)} stations\")\n",
    "print(f\"ğŸ“Š Stations with readings: {len(readings_ids)} stations\")\n",
    "\n",
    "# Find overlaps and differences\n",
    "common_ids = stations_ids & readings_ids\n",
    "stations_only = stations_ids - readings_ids\n",
    "readings_only = readings_ids - stations_ids\n",
    "\n",
    "print(f\"\\nğŸ” DATA RELATIONSHIP ANALYSIS:\")\n",
    "print(f\"   ğŸ¤ Stations with both location AND readings: {len(common_ids)}\")\n",
    "print(f\"   ğŸ“ Stations with location but NO readings: {len(stations_only)}\")\n",
    "print(f\"   ğŸ“Š Stations with readings but NO location: {len(readings_only)}\")\n",
    "\n",
    "if common_ids:\n",
    "    print(f\"\\nâœ… Common stations: {sorted(list(common_ids))}\")\n",
    "    \n",
    "if stations_only:\n",
    "    print(f\"\\nğŸ“ Stations with location only: {sorted(list(stations_only))}\")\n",
    "    \n",
    "if readings_only:\n",
    "    print(f\"\\nğŸ“Š Stations with readings only: {sorted(list(readings_only))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”€ Step 4: Basic Join Operations - Inner Join\n",
    "\n",
    "Let's start with the most restrictive join - an **inner join** that only keeps rows where both datasets have matching station_ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join - only keep matches\n",
    "print(\"ğŸ¤ INNER JOIN - Only Keep Matching Records\\n\")\n",
    "\n",
    "inner_joined = pd.merge(readings_df, stations_df, on='station_id', how='inner')\n",
    "\n",
    "print(f\"ğŸ“Š Original readings: {len(readings_df)}\")\n",
    "print(f\"ğŸ“Š Original stations: {len(stations_df)}\")\n",
    "print(f\"ğŸ“Š After inner join: {len(inner_joined)} rows\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ New columns added from stations:\")\n",
    "new_columns = [col for col in inner_joined.columns if col not in readings_df.columns]\n",
    "for col in new_columns:\n",
    "    print(f\"   + {col}\")\n",
    "\n",
    "print(f\"\\nğŸ” Sample of joined data:\")\n",
    "display(inner_joined[['station_id', 'station_name', 'temperature_c', 'latitude', 'longitude']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”€ Step 5: Left Join - Keep All Readings\n",
    "\n",
    "A **left join** keeps all records from the left DataFrame (readings) and adds matching information from the right DataFrame (stations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join - keep all readings\n",
    "print(\"ğŸ“Š LEFT JOIN - Keep All Readings, Add Station Info Where Available\\n\")\n",
    "\n",
    "left_joined = pd.merge(readings_df, stations_df, on='station_id', how='left')\n",
    "\n",
    "print(f\"ğŸ“Š Original readings: {len(readings_df)}\")\n",
    "print(f\"ğŸ“Š After left join: {len(left_joined)} rows\")\n",
    "print(f\"ğŸ“Š All readings preserved: {len(left_joined) == len(readings_df)}\")\n",
    "\n",
    "# Check for missing station information\n",
    "missing_station_info = left_joined['station_name'].isnull().sum()\n",
    "print(f\"\\nğŸ” Readings with missing station info: {missing_station_info}\")\n",
    "\n",
    "if missing_station_info > 0:\n",
    "    print(\"\\nâš ï¸  Readings without station location data:\")\n",
    "    missing_data = left_joined[left_joined['station_name'].isnull()]\n",
    "    print(missing_data[['station_id', 'date', 'temperature_c']].head())\n",
    "else:\n",
    "    print(\"âœ… All readings have complete station information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”€ Step 6: Right Join - Keep All Stations\n",
    "\n",
    "A **right join** keeps all records from the right DataFrame (stations) and adds matching readings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right join - keep all stations\n",
    "print(\"ğŸ“ RIGHT JOIN - Keep All Stations, Add Readings Where Available\\n\")\n",
    "\n",
    "right_joined = pd.merge(readings_df, stations_df, on='station_id', how='right')\n",
    "\n",
    "print(f\"ğŸ“Š Original stations: {len(stations_df)}\")\n",
    "print(f\"ğŸ“Š After right join: {len(right_joined)} rows\")\n",
    "\n",
    "# Check for stations without readings\n",
    "stations_without_readings = right_joined['temperature_c'].isnull().sum()\n",
    "print(f\"\\nğŸ” Stations without readings: {stations_without_readings}\")\n",
    "\n",
    "if stations_without_readings > 0:\n",
    "    print(\"\\nğŸ“ Stations without temperature data:\")\n",
    "    no_readings = right_joined[right_joined['temperature_c'].isnull()]\n",
    "    display(no_readings[['station_id', 'station_name', 'latitude', 'longitude']].drop_duplicates())\n",
    "else:\n",
    "    print(\"âœ… All stations have temperature readings!\")\n",
    "\n",
    "# Show how many readings each station has\n",
    "print(\"\\nğŸ“Š Readings per station (including stations with 0 readings):\")\n",
    "station_reading_counts = right_joined.groupby(['station_id', 'station_name']).size().reset_index(name='reading_count')\n",
    "display(station_reading_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”€ Step 7: Outer Join - Keep Everything\n",
    "\n",
    "An **outer join** keeps all records from both DataFrames, filling with NaN where data is missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join - keep everything\n",
    "print(\"ğŸŒ OUTER JOIN - Keep All Data from Both Datasets\\n\")\n",
    "\n",
    "outer_joined = pd.merge(readings_df, stations_df, on='station_id', how='outer')\n",
    "\n",
    "print(f\"ğŸ“Š Original readings: {len(readings_df)}\")\n",
    "print(f\"ğŸ“Š Original stations: {len(stations_df)}\")\n",
    "print(f\"ğŸ“Š After outer join: {len(outer_joined)} rows\")\n",
    "\n",
    "# Analyze completeness\n",
    "complete_records = outer_joined.dropna().shape[0]\n",
    "print(f\"\\nğŸ“Š COMPLETENESS ANALYSIS:\")\n",
    "print(f\"   Complete records (no missing data): {complete_records}\")\n",
    "print(f\"   Records with some missing data: {len(outer_joined) - complete_records}\")\n",
    "\n",
    "# Show missing data patterns\n",
    "missing_temp = outer_joined['temperature_c'].isnull().sum()\n",
    "missing_station_name = outer_joined['station_name'].isnull().sum()\n",
    "\n",
    "print(f\"\\nğŸ” Missing data patterns:\")\n",
    "print(f\"   Records missing temperature: {missing_temp}\")\n",
    "print(f\"   Records missing station info: {missing_station_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 8: Choosing the Right Join Type\n",
    "\n",
    "Different join types serve different purposes. Let's understand when to use each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all join types\n",
    "print(\"ğŸ” COMPARING JOIN TYPES:\\n\")\n",
    "\n",
    "join_comparison = pd.DataFrame({\n",
    "    'Join Type': ['Inner', 'Left', 'Right', 'Outer'],\n",
    "    'Rows': [len(inner_joined), len(left_joined), len(right_joined), len(outer_joined)],\n",
    "    'Use Case': [\n",
    "        'Only analyze stations with both location AND readings',\n",
    "        'Analyze all readings, add location where available',\n",
    "        'Analyze all stations, see which have readings',\n",
    "        'Keep everything, analyze completeness'\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(join_comparison)\n",
    "\n",
    "print(\"\\nğŸ¯ FOR OUR ASSIGNMENT:\")\n",
    "print(\"   We'll use LEFT JOIN because we want to:\")\n",
    "print(\"   â€¢ Keep ALL temperature readings (our main data)\")\n",
    "print(\"   â€¢ Add station location information where available\")\n",
    "print(\"   â€¢ Not lose any environmental measurements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Step 9: Validating Join Results\n",
    "\n",
    "After joining, it's important to validate that the operation worked as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the left join (our chosen approach)\n",
    "print(\"âœ… VALIDATING LEFT JOIN RESULTS:\\n\")\n",
    "\n",
    "final_joined = left_joined  # This is our chosen result\n",
    "\n",
    "# Check 1: No rows lost from readings\n",
    "print(f\"ğŸ“Š Row count validation:\")\n",
    "print(f\"   Original readings: {len(readings_df)}\")\n",
    "print(f\"   Joined result: {len(final_joined)}\")\n",
    "print(f\"   âœ… No readings lost: {len(readings_df) == len(final_joined)}\")\n",
    "\n",
    "# Check 2: All original columns preserved\n",
    "original_cols = set(readings_df.columns)\n",
    "final_cols = set(final_joined.columns)\n",
    "preserved_cols = original_cols.issubset(final_cols)\n",
    "print(f\"\\nğŸ“‹ Column validation:\")\n",
    "print(f\"   âœ… All original columns preserved: {preserved_cols}\")\n",
    "\n",
    "# Check 3: New columns added\n",
    "new_cols = final_cols - original_cols\n",
    "print(f\"   ğŸ“ˆ New columns added: {sorted(list(new_cols))}\")\n",
    "\n",
    "# Check 4: Data integrity - station_id consistency\n",
    "print(f\"\\nğŸ” Data integrity checks:\")\n",
    "original_station_ids = set(readings_df['station_id'].unique())\n",
    "final_station_ids = set(final_joined['station_id'].unique())\n",
    "print(f\"   âœ… Station IDs preserved: {original_station_ids == final_station_ids}\")\n",
    "\n",
    "# Check 5: Location data availability\n",
    "complete_location_data = final_joined[['station_name', 'latitude', 'longitude']].notna().all(axis=1).sum()\n",
    "total_readings = len(final_joined)\n",
    "location_coverage = (complete_location_data / total_readings) * 100\n",
    "\n",
    "print(f\"\\nğŸ“ Location data coverage:\")\n",
    "print(f\"   Readings with complete location data: {complete_location_data}/{total_readings} ({location_coverage:.1f}%)\")\n",
    "\n",
    "if location_coverage < 100:\n",
    "    missing_location = final_joined[final_joined['station_name'].isnull()]\n",
    "    print(f\"   âš ï¸  Readings missing location data:\")\n",
    "    print(missing_location['station_id'].value_counts())\n",
    "else:\n",
    "    print(f\"   âœ… All readings have complete location data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 10: Exploring the Joined Dataset\n",
    "\n",
    "Let's explore what we can do with our newly joined dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the richness of joined data\n",
    "print(\"ğŸŒŸ EXPLORING THE JOINED DATASET:\\n\")\n",
    "\n",
    "# Show sample with geographic context\n",
    "print(\"ğŸ” Sample of enriched data:\")\n",
    "sample_cols = ['station_id', 'station_name', 'date', 'temperature_c', 'latitude', 'longitude', 'elevation_m']\n",
    "display(final_joined[sample_cols].head())\n",
    "\n",
    "# Geographic analysis now possible\n",
    "if 'latitude' in final_joined.columns and final_joined['latitude'].notna().any():\n",
    "    print(\"\\nğŸ—ºï¸  Geographic insights now available:\")\n",
    "    \n",
    "    # Temperature by location\n",
    "    geo_stats = final_joined.groupby(['station_id', 'station_name', 'latitude', 'longitude']).agg({\n",
    "        'temperature_c': ['mean', 'count']\n",
    "    }).round(1)\n",
    "    \n",
    "    geo_stats.columns = ['avg_temp', 'reading_count']\n",
    "    geo_stats = geo_stats.reset_index()\n",
    "    \n",
    "    print(\"   ğŸ“Š Temperature by station location:\")\n",
    "    display(geo_stats)\n",
    "    \n",
    "    # Find geographic patterns\n",
    "    if len(geo_stats) > 1:\n",
    "        warmest = geo_stats.loc[geo_stats['avg_temp'].idxmax()]\n",
    "        coolest = geo_stats.loc[geo_stats['avg_temp'].idxmin()]\n",
    "        \n",
    "        print(f\"\\nğŸŒ¡ï¸  Geographic temperature patterns:\")\n",
    "        print(f\"   ğŸ”¥ Warmest: {warmest['station_name']} ({warmest['avg_temp']}Â°C)\")\n",
    "        print(f\"      Location: {warmest['latitude']:.3f}N, {warmest['longitude']:.3f}W\")\n",
    "        print(f\"   â„ï¸  Coolest: {coolest['station_name']} ({coolest['avg_temp']}Â°C)\")\n",
    "        print(f\"      Location: {coolest['latitude']:.3f}N, {coolest['longitude']:.3f}W\")\n",
    "\n",
    "# Temporal patterns with location context\n",
    "if 'date' in final_joined.columns:\n",
    "    print(\"\\nğŸ“… Temporal patterns with geographic context:\")\n",
    "    daily_geo = final_joined.groupby(['date', 'station_name'])['temperature_c'].mean().unstack(fill_value=np.nan)\n",
    "    \n",
    "    if not daily_geo.empty:\n",
    "        print(\"   ğŸ“Š Daily temperature variations across stations:\")\n",
    "        display(daily_geo.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Step 11: Building the Complete Function\n",
    "\n",
    "Now let's put everything together into the complete `join_station_data()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_station_data(stations_df, readings_df):\n",
    "    \"\"\"\n",
    "    Join weather station location data with temperature readings.\n",
    "    \n",
    "    This function demonstrates pandas merge operations to combine related\n",
    "    datasets, enabling geographic analysis of environmental data.\n",
    "    \n",
    "    Args:\n",
    "        stations_df (pandas.DataFrame): Weather station information with columns:\n",
    "            - station_id: Unique station identifier\n",
    "            - station_name: Human-readable station name\n",
    "            - latitude, longitude: Station coordinates\n",
    "            - elevation_m: Station elevation in meters\n",
    "            \n",
    "        readings_df (pandas.DataFrame): Environmental measurements with columns:\n",
    "            - station_id: Links to stations_df station_id\n",
    "            - date: Measurement date\n",
    "            - temperature_c: Temperature measurement in Celsius\n",
    "            - humidity_percent: Humidity measurement percentage\n",
    "            - data_quality: Quality assessment flag\n",
    "            \n",
    "    Returns:\n",
    "        pandas.DataFrame: Combined dataset with both station information \n",
    "                         and environmental readings\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"JOINING STATION DATA WITH READINGS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Input validation\n",
    "    if stations_df is None or stations_df.empty:\n",
    "        print(\"âŒ ERROR: Empty or None stations DataFrame\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    if readings_df is None or readings_df.empty:\n",
    "        print(\"âŒ ERROR: Empty or None readings DataFrame\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_station_cols = ['station_id']\n",
    "    required_reading_cols = ['station_id']\n",
    "    \n",
    "    missing_station_cols = [col for col in required_station_cols if col not in stations_df.columns]\n",
    "    missing_reading_cols = [col for col in required_reading_cols if col not in readings_df.columns]\n",
    "    \n",
    "    if missing_station_cols:\n",
    "        print(f\"âŒ ERROR: Missing columns in stations data: {missing_station_cols}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    if missing_reading_cols:\n",
    "        print(f\"âŒ ERROR: Missing columns in readings data: {missing_reading_cols}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Show input data summary\n",
    "    print(f\"ğŸ“Š INPUT DATA SUMMARY:\")\n",
    "    print(f\"   Stations dataset: {len(stations_df)} stations\")\n",
    "    print(f\"   Readings dataset: {len(readings_df)} readings\")\n",
    "    \n",
    "    # Analyze the relationship between datasets\n",
    "    stations_ids = set(stations_df['station_id'].unique())\n",
    "    readings_ids = set(readings_df['station_id'].unique())\n",
    "    \n",
    "    common_ids = stations_ids & readings_ids\n",
    "    stations_only = stations_ids - readings_ids\n",
    "    readings_only = readings_ids - stations_ids\n",
    "    \n",
    "    print(f\"\\nğŸ” RELATIONSHIP ANALYSIS:\")\n",
    "    print(f\"   ğŸ¤ Stations with both location AND readings: {len(common_ids)}\")\n",
    "    print(f\"   ğŸ“ Stations with location but NO readings: {len(stations_only)}\")\n",
    "    print(f\"   ğŸ“Š Stations with readings but NO location: {len(readings_only)}\")\n",
    "    \n",
    "    if readings_only:\n",
    "        print(f\"\\nâš ï¸  WARNING: Some readings have no station location data:\")\n",
    "        print(f\"   Station IDs without location: {sorted(list(readings_only))}\")\n",
    "        print(f\"   These readings will have missing location information.\")\n",
    "    \n",
    "    # Perform the join (left join to preserve all readings)\n",
    "    print(f\"\\nğŸ”— PERFORMING LEFT JOIN...\")\n",
    "    print(f\"   Using LEFT JOIN to preserve all temperature readings\")\n",
    "    print(f\"   Adding station location data where available\")\n",
    "    \n",
    "    joined_df = pd.merge(readings_df, stations_df, on='station_id', how='left')\n",
    "    \n",
    "    # Validate the join results\n",
    "    print(f\"\\nğŸ“Š JOIN RESULTS:\")\n",
    "    print(f\"   Original readings: {len(readings_df)}\")\n",
    "    print(f\"   After joining: {len(joined_df)} rows\")\n",
    "    print(f\"   âœ… All readings preserved: {len(readings_df) == len
